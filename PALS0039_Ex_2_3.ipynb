{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HanSong19/PALS0039-Introduction-to-Deep-Learning-for-Speech-and-Language-Processing-/blob/main/PALS0039_Ex_2_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVyGSWa50Jli"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "#Exercise 2.3 Classification task\n",
        "\n",
        "In this exercise we train a model to classify vowels from their [formant frequencies](https://en.wikipedia.org/wiki/Formant)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcxVFZU60ONa"
      },
      "source": [
        "The following code reads in and summarises a data set of vowel formant frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etXkZPRl0GTj"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "df = pd.read_csv(\"https://www.phon.ucl.ac.uk/courses/pals0039/data/exercise_02/vowels.csv\")\n",
        "\n",
        "print(df)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(df.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Create three boxplots that show the differences in F1, F2, and HEIGHT between male and female samples.\n",
        "\n",
        "Hint: You could use [`plt.subplots`](https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html) to position all the plots in a row or column. You could use the [`boxplot` method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.boxplot.html) of `DataFrame` to create each plot."
      ],
      "metadata": {
        "id": "POi8JLOK1R2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(a)\n",
        "sexdata= pd.DataFrame(df.groupby(['SEX'])['F1','F2','HEIGHT'].mean())\n",
        "sexdata.reset_index(inplace=True)\n",
        "print(sexdata)"
      ],
      "metadata": {
        "id": "ua-BxNSx2QBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function .query allows to access df and subset of the data\n",
        "f1_female = df.query(\"SEX == 'female'\")['F1']\n",
        "f1_male = df.query(\"SEX == 'male'\")['F1']\n",
        "f1=pd.concat([f1_female, f1_male], axis = 1)\n",
        "f1.columns = ['female', 'male']\n",
        "\n",
        "f2_female= df.query(\"SEX == 'female'\")['F2']\n",
        "f2_male=df.query(\"SEX == 'male'\")['F2']\n",
        "f2=pd.concat([f2_female, f2_male], axis =1)\n",
        "f2.columns=['female', 'male']\n",
        "\n",
        "height_female = df.query(\"SEX == 'female'\")[\"HEIGHT\"]\n",
        "height_male = df.query(\"SEX == 'male'\")[\"HEIGHT\"]\n",
        "height= pd.concat([height_female, height_male], axis=1)\n",
        "height.columns =['female', 'male']\n",
        "print(f1)\n",
        "\n",
        "import seaborn as sns\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize = (15,6))\n",
        "\n",
        "ax1.set_ylabel (\"F1 (Hz)\")\n",
        "f1.boxplot(ax=ax1)\n",
        "ax2.set_ylabel(\"F2 (Hz)\")\n",
        "f2.boxplot(ax=ax2)\n",
        "ax3.set_ylabel(\"HEIGHT (cm)\")\n",
        "height.boxplot(ax=ax3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fPrPfnmi3OWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sexdata= pd.DataFrame(df.groupby(['SEX'])['F1','F2','HEIGHT'].mean())\n",
        "sexdata.reset_index(inplace=True)\n",
        "print(sexdata)\n"
      ],
      "metadata": {
        "id": "ulDim7l-1cf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntNyatxm5Szp"
      },
      "source": [
        "---\n",
        "(b) This code plots an F1-F2 scatter plot in which different vowels are displayed in different colours. Run the code and then add comments to the code to describe what is happening in each step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2N3kjul5-R1"
      },
      "source": [
        "# convert to \"category\" type\n",
        "df[\"VOWEL\"]=df.VOWEL.astype(\"category\")\n",
        "\n",
        "# encode each category with a unique number\n",
        "df[\"VOWELIDX\"]=df.VOWEL.cat.codes\n",
        "\n",
        "# plot formants in dataframe data\n",
        "# data frame must have a column \"VOWELIDX\" to distinguish vowel categories\n",
        "def plot_formants(data, f1=\"F1\", f2=\"F2\", axis_ranges=[3000,500,1100,100]):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.scatter(data[f2], data[f1], c=data.VOWELIDX, cmap=\"tab10\")\n",
        "  if axis_ranges: plt.axis(axis_ranges)\n",
        "  plt.xlabel(\"F2 [Hz]\")\n",
        "  plt.ylabel(\"F1\")\n",
        "  plt.grid()\n",
        "\n",
        "plot_formants(df)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## plot with Pandas Plot\n",
        "\n",
        "#make each vowel into a category\n",
        "df['VOWEL'] = df['VOWEL'].astype('category')\n",
        "#change the strings of category into integers because models cannot\n",
        "#work on categorical variable with the forms of string\n",
        "#df[\"VOWELIDX\"]=df.VOWEL.cat.codes\n",
        "#df\n",
        "\n",
        "df.plot(x='F2', y='F1', c='VOWEL', kind='scatter', cmap='YlGnBu_r')"
      ],
      "metadata": {
        "id": "APU4TtOzF9OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot with matplotlib\n",
        "plt.scatter(x=df['F2'], y=df['F1'], c= df.VOWELIDX, cmap='tab10')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "52nXvI-PRMbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Interpreting the above scatter plot: Will perfect classification be possible using these measurements (F1 and F2 in Hz)? Why?"
      ],
      "metadata": {
        "id": "BESvXCcJ_LFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(c)\n",
        "# No, the scatter plot indicates a large degree of overlap between different vowels, the vowels are not cleanly separable in F1-F2 space."
      ],
      "metadata": {
        "id": "IermhiJV_j4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below randomly selects a small held-out test set (which is plotted). The remaining samples are defined as the training set.\n",
        "\n",
        "(d) Is the test set fully representative of the task? Why?"
      ],
      "metadata": {
        "id": "i8pytWYlBRPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get 5% of the whole data as test_set\n",
        "test_set = df.sample(frac=0.05, random_state=0)\n",
        "print(test_set.describe())\n",
        "\n",
        "#index shows the ordered number of each values (1번, 2번 3번 ...) and drop index together\n",
        "train_set= df.drop(test_set.index)\n",
        "print(train_set.describe())\n",
        "\n",
        "test_set.plot(x='F2', y='F1', c='VOWELIDX', kind='scatter',cmap='Accent')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#(d)\n",
        "# - Not really, we did not ensure that all vowel classes are represented in the test set. 5% of 484 is 24 but we have only 10 classes.\n",
        "# random splits can work in large data sets but be careful when data points are correlated!"
      ],
      "metadata": {
        "id": "aXec9a41DXUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Use `sklearn` to train a [Nearest Neighbour Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) on the training set. The inputs should be `F1` and `F2` and the output should be the `VOWEL`. Configure the classifier to use the 3 nearest neighbours.\n",
        "\n",
        "Hint: You need to define the classifier then call the [`fit` method](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.fit)"
      ],
      "metadata": {
        "id": "ol28m6XAJlHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "neigh.fit(train_set[['F1','F2']], train_set['VOWEL'])\n"
      ],
      "metadata": {
        "id": "qCKcjoeNKMrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(f) Determine the classification accuracy of your classifier on the train and test sets.\n",
        "\n",
        "Hint: You can use the [`score` method](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.score) of the classifier."
      ],
      "metadata": {
        "id": "kj-adFEoLeWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(f)\n",
        "print(\"test set accuracy:\", neigh.score(test_set[['F1','F2']], test_set['VOWEL']), sep=\"\\t\")\n",
        "print(\"train set accuracy:\", neigh.score(train_set[['F1','F2']], train_set['VOWEL']),sep=\"\\t\")\n",
        "\n",
        "## the accuracy of training is higher than that of test (75 vs. 54)\n",
        "## this means that there might have been over-fitting in the training. or the sample was too little\n",
        "## all vowels might not be well represented in the training data"
      ],
      "metadata": {
        "id": "CCS4g-4LLpXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(g) The following code normalises the data using the [z-score](https://en.wikipedia.org/wiki/Standard_score) for each speaker individually.\n",
        "\n",
        "Add comments to each code block to explain what is happening."
      ],
      "metadata": {
        "id": "dy70BjK3OmeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for each speaker, calculate mean and standard deviation across all samples of that speaker\n",
        "mean = df.groupby(['SPEAKER']).mean() #group by speakers so that I can get eash speaker's mean in all values(F1, F2...)\n",
        "std=df.groupby(['SPEAKER']).std()\n",
        "print(mean)\n",
        "print(std)\n",
        "\n"
      ],
      "metadata": {
        "id": "yJ1v5amYPpC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to numpy arrays (ndarray) of F1 and F2 mean\n",
        "# This is because Numpy can be also used in Tensle Flow and it is better to change df of pandas to numpy\n",
        "# added [dr['SPEAKER']] because I need to calculate normalization later and the number of values should match in df. and F1_mean\n",
        "F1_mean=mean['F1'][df['SPEAKER']].to_numpy()\n",
        "F1_std=std['F1'][df['SPEAKER']].to_numpy()\n",
        "F2_mean=mean['F2'][df['SPEAKER']].to_numpy()\n",
        "F2_std=std['F2'][df['SPEAKER']].to_numpy()\n",
        "print(\"F1_mean:\", F1_mean)\n",
        "print(\"F2_std:\", F2_std)\n",
        "\n",
        "\n",
        "# normalise F1 and F2 to have zero mean and unit variance\n",
        "df[\"F1norm\"] = (df.F1 - F1_mean) / F1_std\n",
        "df[\"F2norm\"] = (df.F2 - F2_mean) / F2_std\n",
        "\n",
        "# some general statistics on each column of df\n",
        "print(df.describe())\n"
      ],
      "metadata": {
        "id": "PusHYaHeudil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use plot_formants on normalised axes for f1 and f2\n",
        "df.plot(x=\"F2norm\", y=\"F1norm\", c=\"VOWEL\", kind='scatter')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1vOjQp1buVmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(h) Train and evaluate a new classifier, as before, using the normalised formant data. What was the effect on the classification accuracy? Why?"
      ],
      "metadata": {
        "id": "x9jhqd5vTwZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(h)\n",
        "# statistics about test and training set\n",
        "test_set= df.sample(frac=.05)\n",
        "print(test_set.describe())\n",
        "train_set=df.drop(test_set.index)\n",
        "print(train_set.describe())\n",
        "\n",
        "#here, instead of train_set[['F1','F2']], I use train_set[['F1norm','F2norm']]\n",
        "#because I am testing the normalized ones\n",
        "\n",
        "neig=KNeighborsClassifier(n_neighbors=3)\n",
        "neig.fit(train_set[['F1norm','F2norm']], train_set['VOWEL'])\n",
        "print(\"train data score:\", neig.score(train_set[['F1norm','F2norm']], train_set['VOWEL']))\n",
        "print(\"test data score:\", neig.score(test_set[['F1norm','F2norm']], test_set['VOWEL']))\n",
        "\n",
        "# - The accuracy improved.\n",
        "# this indicates that individuals have different F1 and F2, which affected the model\n",
        "# after normalisation the data is more separable."
      ],
      "metadata": {
        "id": "PyNVmL3XVDne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(i) In this exercise we calculated the statistics for normalisation on all the data (train and test sets combined), is this problematic? What would the consequence be when calculating the generalisation error? When deploying this system, how would we perform this normalisation for a new (unseen) speaker?"
      ],
      "metadata": {
        "id": "qvwTrmD3WFCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(i)\n",
        "# - The generalisation error could be underestimated because the statistics estimates contained the test samples!\n",
        "# maybe not normalize the test_set?\n",
        "# - For an unseen speaker we would need to collect enough data to estimate their statistics first."
      ],
      "metadata": {
        "id": "enZjMEUkWngw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(j) In this exercise we did not make use of a validation set, was it necessary? Why?"
      ],
      "metadata": {
        "id": "x8oG8adGWoCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(f)\n",
        "# - The use of a validation set would have been necessary if we wanted to find better hyperparameters for the classifier\n",
        "#   (e.g. it is possible that using a larger number of neighbours could result in better generalisation error)"
      ],
      "metadata": {
        "id": "ga2JTudZWqse"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}