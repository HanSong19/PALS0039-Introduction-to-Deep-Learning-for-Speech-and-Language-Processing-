{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HanSong19/PALS0039-Introduction-to-Deep-Learning-for-Speech-and-Language-Processing-/blob/main/pals0039_ex_2_2_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCvRvja7Yv9m"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "#Exercise 2.2 Regression task\n",
        "\n",
        "In this exercise we set up a simple regression task and explore how well different regression models fit the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olG5JLfUY26F"
      },
      "source": [
        "(a) The following code generates some random training and test data for a regression problem. Run the code, then add comments to explain the different steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfaQZe_7Yu6x"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# generate always the same random numbers by setting the seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# 1st order polynomial (a line)\n",
        "def linear_polynomial(x, gradient=4.0, intercept=2.0):\n",
        "  return gradient * x + intercept                                               # comments are also good here\n",
        "\n",
        "# generate noisy samples around a line\n",
        "# num_samples: number of samples to generate\n",
        "# domain: input range for which samples are generated (default: 0-1)\n",
        "# output: x and y\n",
        "def generate_noisy_samples(num_samples, domain=[0.0, 1.0]):\n",
        "  x = np.linspace(domain[0], domain[1], num_samples)                            # num_samples equidistant numbers between domain[0] and domain[1]\n",
        "  y = linear_polynomial(x) + np.random.normal(size=num_samples)                 # sum of line and noise\n",
        "  return x, y\n",
        "\n",
        "# generate 100 samples for training and 100 for testing\n",
        "x_train, y_train = generate_noisy_samples(100)\n",
        "x_test, y_test = generate_noisy_samples(100)\n",
        "\n",
        "(print(x_train, y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the noisy training samples and the noise-free ground truth\n",
        "plt.plot(x_train, y_train,'bo', label=\"noisy training samples\")\n",
        "plt.plot(x_train, linear_polynomial(x_train), label=\"true relationship\", \n",
        "         linestyle=\"--\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "xSTmXeBIn8HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Use the numpy [`Polynomial.fit` method](https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.fit.html) to find a polynomial with **degree of 0** from the training data. Plot this function against the training data (as above).\n",
        "\n",
        "Hint: to evaluate the estimated model returned by `Polynomial.fit` you can simply use it as a callable function that takes x values as input."
      ],
      "metadata": {
        "id": "YFphmm9HOmKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from numpy.polynomial import Polynomial\n",
        "\n",
        "#(b)\n",
        "\n",
        "plot0=Polynomial.fit(x_train, y_train, deg=0)\n",
        "\n",
        "print(\"x_train:\", x_train)\n",
        "print(\"y_train:\", y_train)\n",
        "print(\"plot0\", plot0)\n",
        "print(\"plot0(x_train):\", plot0(x_train))\n",
        "plt.plot(x_train, y_train, 'bo', label=\"noisy training samples\")\n",
        "plt.plot(x_train, linear_polynomial(x_train),linestyle='--',label=\"true relationship\")\n",
        "plt.plot(x_train, plot0(x_train), label=\"deg 0 polynomial train\")\n",
        "\n",
        "plt.legend() \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7C7g7iL8PTa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Repeat the previous task but fit two additional models with **degree 1 and 2** respectively."
      ],
      "metadata": {
        "id": "_Nx3dM1yP5w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(c)\n",
        "plot1=Polynomial.fit(x_train, y_train, deg=1)\n",
        "plot2=Polynomial.fit(x_train, y_train, deg=2)\n",
        "plt.plot(x_train, y_train, 'bo', label=\"noisy training samples\")\n",
        "plt.plot(x_train, linear_polynomial(x_train),linestyle='--',label=\"true relationship\")\n",
        "plt.plot(x_train, plot0(x_train), label=\"deg 0 polynomial\")\n",
        "plt.plot(x_train, plot1(x_train), label=\"deg 1 polynomial\")\n",
        "plt.plot(x_train, plot2(x_train), label='deg2 polynomial')\n",
        "plt.legend() \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QeYpKkcOQIqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqJhR7pyZUiU"
      },
      "source": [
        "(d) Calculate the **mean squared error** (MSE) on both the training and test sets for the three polynomial models.\n",
        "\n",
        "Hint: SKLearn has a function that can be used to [calculate the MSE](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ9SAPXbZixM"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#(d)\n",
        "mse0= mean_squared_error(plot0(x_train),y_train)\n",
        "mse1= mean_squared_error(plot1(x_train), y_train)\n",
        "mse2= mean_squared_error(plot2(x_train), y_train)\n",
        "print(\"train plot0:\", mse0,\"plot1:\",mse1, \"plot2:\",mse2)\n",
        "\n",
        "mset0=mean_squared_error(plot0(x_test),y_test)\n",
        "mset1=mean_squared_error(plot1(x_test),y_test)\n",
        "mset2=mean_squared_error(plot2(x_test),y_test)\n",
        "print(\"test plot0:\", mset0,\"plot1:\",mset1, \"plot2:\",mset2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Can you identify underfitting and overfitting? Which model would you eventually deploy on this task? Why?"
      ],
      "metadata": {
        "id": "MmqEUoJOY2jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(e)\n",
        "# Underfitting for poly0 (able to achieve better generalisation error with poly1 and poly2)\n",
        "# Overfitting for poly2 (training error was lower than poly1 but generalisation error was higher)\n",
        "# Deploy poly1 (best generalisation error)"
      ],
      "metadata": {
        "id": "arU008pLZHhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(f) Pick a regression method of your choice from the `sklearn` library (regression models with example code can be found in [this index](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)). Identify some of the hyperparameters of this model and try to find out how they affect the modelling."
      ],
      "metadata": {
        "id": "QQ_G4mdIZTCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(f)\n",
        "# DecisionTreeRegressor: max_depth, min_samples_split, min_samples_leaf, ..."
      ],
      "metadata": {
        "id": "Z52DnGTIaKWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(g) Fit your choice of regression model with two or three different configurations of the hyperparamers and calculate the train and test set MSE for this model (as done before). Which model would you eventually deploy on this task?"
      ],
      "metadata": {
        "id": "dyxZdSBPaUCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(g)\n",
        "from sklearn import tree\n",
        "\n",
        "treedegree2=tree.DecisionTreeRegressor(max_depth=2).fit(x_train.reshape(-1,1), y_train)\n",
        "treedegree4=tree.DecisionTreeRegressor(max_depth=4).fit(x_train.reshape(-1,1), y_train)\n",
        "treedegree8=tree.DecisionTreeRegressor(max_depth=8).fit(x_train.reshape(-1,1), y_train)\n",
        "\n",
        "mse_train={}\n",
        "mse_test={}\n",
        "\n",
        "mse_train['tree_D2'] = mean_squared_error(treedegree2.predict(x_train.reshape(-1,1)), y_train)\n",
        "mse_train['tree_D4'] = mean_squared_error(treedegree4.predict(x_train.reshape(-1,1)), y_train)\n",
        "mse_train['tree_D8'] = mean_squared_error(treedegree8.predict(x_train.reshape(-1,1)), y_train)\n",
        "\n",
        "mse_test['tree_D2'] = mean_squared_error(treedegree2.predict(x_test.reshape(-1,1)), y_test)\n",
        "mse_test['tree_D4'] = mean_squared_error(treedegree4.predict(x_test.reshape(-1,1)), y_test)\n",
        "mse_test['tree_D8'] = mean_squared_error(treedegree8.predict(x_test.reshape(-1,1)), y_test)\n",
        "\n",
        "print(\"train:\", mse_train, \"test:\", mse_test, sep=\"\\t\")\n",
        "\n",
        "plt.plot(x_train,y_train, 'bo')\n",
        "label=\"noisy training sample\"\n",
        "plt.plot(x_train, linear_polynomial(x_train), label='true relationship', linestyle= '--')\n",
        "plt.plot(x_train,treedegree2.predict(x_train.reshape(-1,1)), label= 'tree depth=2')\n",
        "plt.plot(x_train,treedegree4.predict(x_train.reshape(-1,1)), label= 'tree depth=4')\n",
        "plt.plot(x_train,treedegree8.predict(x_train.reshape(-1,1)), label= 'tree depth=8')\n",
        "\n",
        "plt.legend()\n",
        "plt.title(label)\n",
        "plt.show()\n",
        "\n",
        "#Deploy tree_d2 (best generalisation error) -- other models exhibited overfitting!"
      ],
      "metadata": {
        "id": "zuW-LI3marDE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}