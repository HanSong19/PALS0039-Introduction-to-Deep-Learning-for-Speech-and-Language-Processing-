{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HanSong19/PALS0039-Introduction-to-Deep-Learning-for-Speech-and-Language-Processing-/blob/main/PALS0039_Ex_2_3_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVyGSWa50Jli"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "#Exercise 2.3 Classification task\n",
        "\n",
        "In this exercise we train a model to classify vowels from their [formant frequencies](https://en.wikipedia.org/wiki/Formant)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcxVFZU60ONa"
      },
      "source": [
        "The following code reads in and summarises a data set of vowel formant frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etXkZPRl0GTj"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "df = pd.read_csv(\"https://www.phon.ucl.ac.uk/courses/pals0039/data/exercise_02/vowels.csv\")\n",
        "\n",
        "print(df)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(df.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Create three boxplots that show the differences in F1, F2, and HEIGHT between male and female samples.\n",
        "\n",
        "Hint: You could use [`plt.subplots`](https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html) to position all the plots in a row or column. You could use the [`boxplot` method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.boxplot.html) of `DataFrame` to create each plot."
      ],
      "metadata": {
        "id": "POi8JLOK1R2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(a)\n",
        "f1_male = df.query('SEX ==\"male\"')[\"F1\"]                  # function .query is a data base acccess and it gives access to the subset of the data\n",
        "f1_female = df.query('SEX ==\"female\"')[\"F1\"]\n",
        "f1 = pd.concat([f1_male, f1_female], axis=1)\n",
        "f1.columns = [\"male\", \"female\"]\n",
        "\n",
        "f2_male = df.query('SEX ==\"male\"')[\"F2\"]\n",
        "f2_female = df.query('SEX ==\"female\"')[\"F2\"]\n",
        "f2 = pd.concat([f2_male, f2_female], axis=1)\n",
        "f2.columns = [\"male\", \"female\"]\n",
        "\n",
        "height_male = df.query('SEX ==\"male\"')[\"HEIGHT\"]\n",
        "height_female = df.query('SEX ==\"female\"')[\"HEIGHT\"]\n",
        "height = pd.concat([height_male, height_female], axis=1)\n",
        "height.columns = [\"male\", \"female\"]\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n",
        "axs[0].set_ylabel(\"F1 (Hz)\")\n",
        "f1.boxplot(ax=axs[0])\n",
        "axs[1].set_ylabel(\"F2 (Hz)\")\n",
        "f2.boxplot(ax=axs[1])\n",
        "axs[2].set_ylabel(\"HEIGHT (cm)\")\n",
        "height.boxplot(ax=axs[2])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ua-BxNSx2QBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntNyatxm5Szp"
      },
      "source": [
        "---\n",
        "(b) This code plots an F1-F2 scatter plot in which different vowels are displayed in different colours. Run the code and then add comments to the code to describe what is happening in each step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2N3kjul5-R1"
      },
      "source": [
        "# convert to \"category\" type\n",
        "df[\"VOWEL\"]=df.VOWEL.astype(\"category\")\n",
        "\n",
        "# encode each category with a unique number\n",
        "df[\"VOWELIDX\"]=df.VOWEL.cat.codes\n",
        "\n",
        "# plot formants in dataframe data\n",
        "# data frame must have a column \"VOWELIDX\" to distinguish vowel categories\n",
        "def plot_formants(data, f1=\"F1\", f2=\"F2\", axis_ranges=[3000,500,1100,100]):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.scatter(data[f2], data[f1], c=data.VOWELIDX, cmap=\"tab10\")\n",
        "  if axis_ranges: plt.axis(axis_ranges)\n",
        "  plt.xlabel(\"F2\")\n",
        "  plt.ylabel(\"F1\")\n",
        "  plt.grid()\n",
        "\n",
        "plot_formants(df)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Interpreting the above scatter plot: Will perfect classification be possible using these measurements (F1 and F2 in Hz)? Why?"
      ],
      "metadata": {
        "id": "BESvXCcJ_LFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(c)\n",
        "# No, the scatter plot indicates a large degree of overlap between different vowels, the vowels are not cleanly separable in F1-F2 space."
      ],
      "metadata": {
        "id": "IermhiJV_j4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below randomly selects a small held-out test set (which is plotted). The remaining samples are defined as the training set.\n",
        "\n",
        "(d) Is the test set fully representative of the task? Why?"
      ],
      "metadata": {
        "id": "i8pytWYlBRPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = df.sample(frac=0.05, random_state=0)                                 #frac=5% of the entire data\n",
        "print(test_set.describe())\n",
        "\n",
        "train_set = df.drop(test_set.index)\n",
        "print(train_set.describe())\n",
        "\n",
        "plot_formants(test_set)\n",
        "plt.show()\n",
        "\n",
        "#(d)\n",
        "# - Not really, we did not ensure that all vowel classes are represented in the test set. 5% of 484 is 24 but we have only 10 classes.\n",
        "# 10 classes might be represented in 24 dataset but it might not either.\n",
        "# random splits can work in large data sets but be careful when data points are correlated!"
      ],
      "metadata": {
        "id": "aXec9a41DXUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Use `sklearn` to train a [Nearest Neighbour Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) on the training set. The inputs should be `F1` and `F2` and the output should be the `VOWEL`. Configure the classifier to use the 3 nearest neighbours.\n",
        "\n",
        "Hint: You need to define the classifier then call the [`fit` method](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.fit)"
      ],
      "metadata": {
        "id": "ol28m6XAJlHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "#Nearest Neighbour Classifier: look into what is the closest value. maching learning method\n",
        "#(e)\n",
        "clf = KNeighborsClassifier(n_neighbors=3)                                       #look at 3 neareest neighbours around me.\n",
        "clf.fit(train_set[[\"F1\", \"F2\"]], train_set[\"VOWEL\"])                            # only do it with the training set! not the test set"
      ],
      "metadata": {
        "id": "qCKcjoeNKMrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(f) Determine the classification accuracy of your classifier on the train and test sets.\n",
        "\n",
        "Hint: You can use the [`score` method](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.score) of the classifier."
      ],
      "metadata": {
        "id": "kj-adFEoLeWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(f)\n",
        "print(\"TRAIN SET ACCURACY:\", clf.score(train_set[[\"F1\", \"F2\"]], train_set[\"VOWEL\"]), sep=\"\\t\")\n",
        "print(\"TEST SET ACCURACY:\", clf.score(test_set[[\"F1\", \"F2\"]], test_set[\"VOWEL\"]), sep=\"\\t\")\n",
        "# it is better for the training set but the test set is only 54. It could be over-fitting or it could be because the sample was too little\n",
        "# and all catetories might not be all represented in the training data"
      ],
      "metadata": {
        "id": "CCS4g-4LLpXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(g) The following code normalises the data using the [z-score](https://en.wikipedia.org/wiki/Standard_score) for each speaker individually.\n",
        "\n",
        "Add comments to each code block to explain what is happening."
      ],
      "metadata": {
        "id": "dy70BjK3OmeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize to gausian distribution by setting the mean =0 and deviation=1\n",
        "# for each speaker, calculate mean and standard deviation across all samples of that speaker\n",
        "means = df.groupby(['SPEAKER']).agg(\"mean\")                                     #I group it by speakers and then for each speaker I take the mean\n",
        "stds = df.groupby(['SPEAKER']).agg(\"std\")\n",
        "\n",
        "# convert to numpy arrays (ndarray)                                             # numpy data type can be used in other areas such as tensle flow so it is good to change to numpy from Pandas\n",
        "F1mean = means.F1[df.SPEAKER].to_numpy()                                        \n",
        "F1std = stds.F1[df.SPEAKER].to_numpy()\n",
        "F2mean = means.F2[df.SPEAKER].to_numpy()\n",
        "F2std = stds.F2[df.SPEAKER].to_numpy()\n",
        "\n",
        "# normalise F1 and F2 to have zero mean and unit variance\n",
        "df[\"F1norm\"] = (df.F1 - F1mean) / F1std\n",
        "df[\"F2norm\"] = (df.F2 - F2mean) / F2std\n",
        "\n",
        "# some general statistics on each column of df\n",
        "print(df.describe())\n",
        "\n",
        "# use plot_formants on normalised axes for f1 and f2\n",
        "plot_formants(df, f1=\"F1norm\", f2=\"F2norm\", axis_ranges=None)\n",
        "plt.show()\n",
        "\n",
        "# statistics about test and training set\n",
        "test_set = df.sample(frac=0.05, random_state=0)\n",
        "print(test_set.describe())\n",
        "train_set = df.drop(test_set.index)\n",
        "print(train_set.describe())"
      ],
      "metadata": {
        "id": "yJ1v5amYPpC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we normalize the test set so I am not supposed to normalize the test data.\n"
      ],
      "metadata": {
        "id": "-1Xhj_aej4wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(h) Train and evaluate a new classifier, as before, using the normalised formant data. What was the effect on the classification accuracy? Why?"
      ],
      "metadata": {
        "id": "x9jhqd5vTwZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(h)\n",
        "clf = KNeighborsClassifier(n_neighbors=3)\n",
        "clf.fit(train_set[[\"F1norm\", \"F2norm\"]], train_set[\"VOWEL\"])\n",
        "\n",
        "print(\"TRAIN SET ACCURACY:\", clf.score(train_set[[\"F1norm\", \"F2norm\"]], train_set[\"VOWEL\"]), sep=\"\\t\")\n",
        "print(\"TEST SET ACCURACY:\", clf.score(test_set[[\"F1norm\", \"F2norm\"]], test_set[\"VOWEL\"]), sep=\"\\t\")\n",
        "\n",
        "# - The accuracy improved.\n",
        "# - Each speaker has slightly different F1 and F2 ranges (depending on their vocal tract) -- after normalisation the data is more separable."
      ],
      "metadata": {
        "id": "PyNVmL3XVDne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(i) In this exercise we calculated the statistics for normalisation on all the data (train and test sets combined), is this problematic? What would the consequence be when calculating the generalisation error? When deploying this system, how would we perform this normalisation for a new (unseen) speaker?"
      ],
      "metadata": {
        "id": "qvwTrmD3WFCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(i)\n",
        "# - The generalisation error could be underestimated because the statistics estimates contained the test samples!\n",
        "# - For an unseen speaker we would need to collect enough data to estimate their statistics first."
      ],
      "metadata": {
        "id": "enZjMEUkWngw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(j) In this exercise we did not make use of a validation set, was it necessary? Why?"
      ],
      "metadata": {
        "id": "x8oG8adGWoCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(f)\n",
        "# - The use of a validation set would have been necessary if we wanted to find better hyperparameters for the classifier\n",
        "#   (e.g. it is possible that using a larger number of neighbours could result in better generalisation error)"
      ],
      "metadata": {
        "id": "ga2JTudZWqse"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}